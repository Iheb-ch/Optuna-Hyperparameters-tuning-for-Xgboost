{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optuna Hyperparameters Tuning for XGBoost.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snCv0NoesUAy"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier,XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error,accuracy_score\n",
        "import optuna"
      ],
      "metadata": {
        "id": "dFd3QSbipNpp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Regression"
      ],
      "metadata": {
        "id": "dg23CmZ_nFqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial,data,target):\n",
        "    \n",
        "    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n",
        "    \n",
        "    # To select which parameters to optimize, please look at the XGBoost documentation:\n",
        "    # https://xgboost.readthedocs.io/en/latest/parameter.html\n",
        "    param = {\n",
        "        'tree_method':'gpu_hist',  # Use GPU acceleration\n",
        "        'lambda': trial.suggest_loguniform(\n",
        "            'lambda', 1e-3, 10.0\n",
        "        ),\n",
        "        'alpha': trial.suggest_loguniform(\n",
        "            'alpha', 1e-3, 10.0\n",
        "        ),\n",
        "        'colsample_bytree': trial.suggest_categorical(\n",
        "            'colsample_bytree', [0.5,0.6,0.7,0.8,0.9,1.0]\n",
        "        ),\n",
        "        'subsample': trial.suggest_categorical(\n",
        "            'subsample', [0.6,0.7,0.8,1.0]\n",
        "        ),\n",
        "        'learning_rate': trial.suggest_categorical(\n",
        "            'learning_rate', [0.001,0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]\n",
        "        ),\n",
        "        'n_estimators': trial.suggest_categorical(\n",
        "            \"n_estimators\", [150, 200, 300, 3000]\n",
        "        ),\n",
        "        'max_depth': trial.suggest_categorical(\n",
        "            'max_depth', [4,5,7,9,11,13,15,17]\n",
        "        ),\n",
        "        'random_state': 42,\n",
        "        'min_child_weight': trial.suggest_int(\n",
        "            'min_child_weight', 1, 300\n",
        "        ),\n",
        "    }\n",
        "    model = XGBRegressor(**param)  \n",
        "    \n",
        "    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n",
        "    \n",
        "    preds = model.predict(test_x)\n",
        "    \n",
        "    rmse = mean_squared_error(test_y, preds,squared=False)\n",
        "    \n",
        "    return rmse"
      ],
      "metadata": {
        "id": "e8uq6DNmsWPo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=5)\n",
        "print('Number of finished trials:', len(study.trials))\n",
        "print('Best trial:', study.best_trial.params)"
      ],
      "metadata": {
        "id": "uk7L_BgLtAPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Classification"
      ],
      "metadata": {
        "id": "0JHFo7tWnIad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial,data,target):\n",
        "    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n",
        "# params specifies the XGBoost hyperparameters to be tuned\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 400, 600,1000,3000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 10, 20),\n",
        "        'learning_rate': trial.suggest_uniform('learning_rate', 0.01,0.07, .1),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n",
        "        'gamma': trial.suggest_int('gamma', 0, 10),\n",
        "        'tree_method': 'gpu_hist',  \n",
        "        'objective': 'binary:logistic'\n",
        "    }\n",
        "    \n",
        "    model = XGBRClassifier(**params)  \n",
        "    \n",
        "    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n",
        "    \n",
        "    preds = model.predict(test_x)\n",
        "# trials will be evaluated based on their accuracy on the test set\n",
        "    accuracy = accuracy_score(test_y, preds,)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "QvxMJR78tMgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=5)\n",
        "print('Number of finished trials:', len(study.trials))\n",
        "print('Best trial:', study.best_trial.params)"
      ],
      "metadata": {
        "id": "AdaX9VJ4nle-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}